# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================

# Port for the superglue REST API server
API_PORT=3002

# Endpoint for the REST API (used so the web dashboard knows where to find the server)
API_ENDPOINT=http://localhost:3002

# Port for the web dashboard
WEB_PORT=3001

# Public URL of the superglue web app (used for notification links)
SUPERGLUE_APP_URL=http://localhost:3001

# Backend authentication token for API access (set when self hosting)
AUTH_TOKEN=your-secret-token

# Frontend authentication token for API access (set when self hosting)
NEXT_PUBLIC_SUPERGLUE_API_KEY=your-secret-token

# Controls whether the workflow scheduler should run alongside superglue.
# ⚠️ Important: Only enable this on a single instance.
# Running multiple schedulers (e.g. in production or when using the same DB)
# can cause conflicts.
START_SCHEDULER_SERVER=false


# ==============================================================================
# DATASTORE
# ==============================================================================

# Datastore type (postgres)
DATASTORE_TYPE=postgres

# PostgreSQL connection settings
POSTGRES_HOST=localhost # 'postgres' when self hosted with docker-compose
POSTGRES_PORT=5432
POSTGRES_USERNAME=superglue
POSTGRES_PASSWORD=your-secure-password
POSTGRES_DB=superglue
# POSTGRES_SSL=false


# ==============================================================================
# LLM PROVIDERS
# ==============================================================================

# AI Provider - OPENAI, GEMINI, ANTHROPIC, BEDROCK, VERTEX, or AZURE
# Best performance/price ratio right now is GEMINI with gemini-2.5-flash
LLM_PROVIDER=GEMINI

# Optional fallback provider if primary fails
# LLM_FALLBACK_PROVIDER=openai

# LLM Provider and Model for the Agent Chat
# More information at https://ai-sdk.dev/docs/foundations/providers-and-models
FRONTEND_LLM_PROVIDER=anthropic

# AI Gateway by Vercel API Key and provider/Model
# Check out all models at https://vercel.com/ai-gateway/models?type=chat
# Leave these empty if you don't want to use the Gateway
# AI_GATEWAY_MODEL=anthropic/claude-sonnet-4.5
# AI_GATEWAY_API_KEY=vck_XXXX

# If GEMINI: Your Google API key
# You can get one here: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=XXXXXXX
GEMINI_MODEL=gemini-2.5-flash

# If OPENAI: Your OpenAI API key
# You can get one here: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-XXXXXXXX
OPENAI_MODEL=gpt-4.1
# Optional: Set a custom OpenAI API URL (for self-hosted models or providers like fireworks.ai)
# For fireworks, use https://api.fireworks.ai/inference/v1
OPENAI_BASE_URL=https://api.openai.com/v1

# If ANTHROPIC: Your API key
# You can get one here: https://docs.anthropic.com/en/api/admin-api/apikeys/get-api-key
ANTHROPIC_API_KEY=sk-ant-XXXXXXX
ANTHROPIC_MODEL=claude-sonnet-4-5

# If AZURE: Azure OpenAI
# More info: https://ai-sdk.dev/providers/ai-sdk-providers/azure
AZURE_API_KEY=your-azure-api-key
# Option 1: Use resource name (Azure will construct URL automatically)
AZURE_RESOURCE_NAME=your-resource-name
# Option 2: Use full base URL (either AZURE_RESOURCE_NAME or AZURE_BASE_URL required)
# Example: https://your-resource.openai.azure.com
AZURE_BASE_URL=https://{resource_name}.openai.azure.com/openai
AZURE_API_VERSION=2025-01-01-preview
# Set to true to use the legacy deployment format:
# {baseURL}/deployments/{deploymentId}{path} instead of {baseURL}/v1{path}
AZURE_USE_DEPLOYMENT_BASED_URLS=false

# If BEDROCK: AWS Bedrock
# More info: https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html
# Disclaimer: superglue is not tested on all models available in Bedrock.
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
AWS_REGION=us-east-1
# Model IDs: https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html
BEDROCK_MODEL=anthropic.claude-3-5-sonnet-20240620-v1:0
# AWS_SESSION_TOKEN=optional_session_token
# AWS_BASE_URL=optional_custom_endpoint

# If VERTEX: Google Vertex AI
# Supports both Gemini and Claude models. Auth method depends on the model:
#
# --- Gemini models (e.g., gemini-2.5-flash) ---
# Uses Express Mode with API key (simplest setup):
# VERTEX_API_KEY=YOUR_VERTEX_API_KEY
# Note: API key auth is ONLY available for Gemini models on Vertex, not Claude.
#
# --- Claude models (e.g., claude-sonnet-4-5) ---
# Requires GCP service account auth. Use ONE of:
# Option A: Inline credentials
#   VERTEX_CLIENT_EMAIL=your-sa@your-project.iam.gserviceaccount.com
#   VERTEX_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
# Option B: Service account key file path
#   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
#
# VERTEX_MODEL=gemini-2.5-flash
# VERTEX_PROJECT=your-project-id
# VERTEX_LOCATION=us-central1

# If using Tavily for web searching
TAVILY_API_KEY=YOUR_API_KEY


# ==============================================================================
# FILE STORAGE
# ==============================================================================

# File storage provider: 'aws' for Amazon S3, 'minio' for self-hosted MinIO
# FILE_STORAGE_PROVIDER=minio

# --- AWS S3 (FILE_STORAGE_PROVIDER=aws) ---
# AWS_BUCKET_NAME=your-bucket-name
# AWS_BUCKET_PREFIX=raw

# --- MinIO (FILE_STORAGE_PROVIDER=minio) ---
# Internal endpoint for backend → MinIO communication
# S3_ENDPOINT=http://localhost:9000
# Public endpoint for browser uploads (presigned URLs)
# S3_PUBLIC_ENDPOINT=http://localhost:9000
# MINIO_ROOT_USER=minioadmin
# MINIO_ROOT_PASSWORD=minioadmin
# MINIO_BUCKET_NAME=superglue-files
# MINIO_BUCKET_PREFIX=raw
# MINIO_API_PORT=9000
# MINIO_CONSOLE_PORT=9001
# Webhook endpoint for MinIO → superglue file processing notifications
# MINIO_WEBHOOK_ENDPOINT=http://host.docker.internal:3002/v1/file-references/process
# Webhook auth token - set this to a valid superglue API key
# MINIO_WEBHOOK_AUTH_TOKEN=your-api-key


# ==============================================================================
# MISC
# ==============================================================================

# Disable the welcome/onboarding screen for development
NEXT_PUBLIC_DISABLE_WELCOME_SCREEN=false

# Used to encrypt credentials at rest - use a strong key and don't lose it, there is no recovery option
# Generate a strong key: openssl rand -hex 32
MASTER_ENCRYPTION_KEY=your-32-byte-encryption-key

# Langfuse (optional - enables LLM observability for the agent)
# LANGFUSE_SECRET_KEY=sk-lf-...
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_BASEURL=https://cloud.langfuse.com
# LANGFUSE_TRACING_ENVIRONMENT=dev
